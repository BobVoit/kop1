{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60cedc72-75a3-4570-8647-ad4ac6cfdc7f",
   "metadata": {},
   "source": [
    "# Сеть встречного распространения\n",
    "\n",
    "## Введение\n",
    "\n",
    "Возможности сети встречного распространения превосходят возможности однослойных сетей. Время же обучения по сравнению с обратным распространением может уменьшаться в сто раз. Встречное распространение не столь общо, как обратное распространение, но оно может давать решение в тех приложениях, где долгая обучающая процедура невозможна. Помимо преодоления ограничений других сетей встречное распространение обладает собственными интересными и полезными свойствами.\n",
    "\n",
    "В даннной сети используются 2 других алгоритма \n",
    "* самоорганизующаяся карта Кохонена\n",
    "* звезда Гроссберка\n",
    "\n",
    "Объединение этих алгоритмов ведет к свойствам, которых нет ни у одного из них в отдельности.\n",
    "\n",
    "Сеть встречного распространения функционирует подобно столу справок, способному к обобщению. В процессе обучения входные векторы ассоциируются с соответствующими выходными векторами. Эти векторы могут быть двоичными, состоящими из нулей и единиц, или непрерывными. Когда сеть обучена, приложение входного вектора приводит к требуемому выходному вектору. Обобщающая способность сети позволяет получать правильный выход даже при приложении входного вектора, который является неполным или слегка неверным. Это позволяет использовать данную сеть для распознавания образов, восстановления образов и усиления сигналов.\n",
    "\n",
    "## Структура сети встречного распространения\n",
    "\n",
    "Иллюстрация функциональных свойств сети встречного распространения без обратных связей.\n",
    "\n",
    "<img src=\"rnn.png\" width=\"500\"/>\n",
    "\n",
    "## Устройство сети\n",
    "\n",
    "Нейроны слоя 0 (показаны кружками) служат лишь точками разветвления и не выполняют вычислений. Каждый нейрон слоя 0 соединен с каждым нейроном слоя 1 (называемого слоем Кохонена) отдельным весом wmn. Эти веса в целом рассматриваются как матрица весов W. Аналогично, каждый нейрон в слое Кохонена (слое 1) соединен с каждым нейроном в слое Гроссберга (слое 2) весом vnp. Эти веса образуют матрицу весов V. Отличие этих сетей состоит в операциях, выполняемых нейронами Кохонена и Гроссберга. \n",
    "\n",
    "Как и многие другие сети, встречное распространение функционирует в двух режимах: в нормальном режиме, при котором принимается входной вектор Х и выдается выходной вектор У, и в режиме обучения, при котором подается входной вектор и веса корректируются, чтобы дать требуемый выходной вектор.\n",
    "\n",
    "### Функционирование сети встречного распространения\n",
    "\n",
    "**Слои Кохоненна.** В своей простейшей форме слой Кохонена функционирует в духе «победитель забирает все», т. е. для данного входного вектора один и только один нейрон Кохонена выдает на выходе логическую единицу, все остальные выдают нуль. Нейроны Кохонена можно воспринимать как набор электрических лампочек, так что для любого входного вектора загорается одна из них. \n",
    "\n",
    "Ассоциированное с каждым нейроном Кохонена множество весовсоединяет его с каждым входом. Например, на рисунке нейрон Кохонена К<sub>1</sub> имеет веса w<sub>11</sub>, w<sub>21</sub>, ., w<sub>m1</sub>, составляющие весовой вектор W<sub>1</sub>. Они соединяются через входной слой с входными сигналами х<sub>1</sub>, x<sub>2</sub>, ., x<sub>rn</sub>, составляющими входной вектор Х. Подобно нейронам большинства сетей выход каждого нейрона Кохонена является просто суммой взвешенных входов.\n",
    "\n",
    "Это может быть выражено следующим образом:\n",
    "\n",
    "<img src=\"expression1.png\" width=\"300\"/>\n",
    "\n",
    "где N - это выход N нейрона Кохонена j <img src=\"expression2.png\" width=\"100\"/>, или в векторной записи **N = X * W** где N - вектор выходов N<sub>j</sub> слоя Кохонена.\n",
    "\n",
    "Нейрон Кохонена с максимальным значением N является «победителем». Его выход равен единице, у остальных он равен нулю.\n",
    "\n",
    "**Слой Гроссберга.** Слой Гроссберга функционирует в сходной манере. Его выход У является взвешенной суммой выходов k<sub>1</sub>, k<sub>2</sub>, ..., k<sub>n</sub> слоя Кохонена, образующих вектор К. Вектор соединяющих весов, обозначенный через V, состоит из весов v<sub>11</sub>, v<sub>21</sub>, ..., v<sub>np</sub>.\n",
    "\n",
    "Тогда выход У<sub>j</sub> каждого нейрона Гроссберга есть\n",
    "\n",
    "<img src=\"expression3.png\" width=\"150\"/>\n",
    "\n",
    "где Y<sub>j</sub> - выход j-го нейрона Гроссберга, или\n",
    "в векторной форме **Y = K * V**, где Y - выходной вектор слоя Гроссберга; К - выходной вектор слоя Кохонена; V - матрица весов слоя Гроссберга. \n",
    "\n",
    "Если слой Кохонена функционирует таким образом, что лишь у одного нейрона величина выхода равна единице, а у остальных равна нулю, то лишь один элемент вектора К отличен от нуля, и вычисления очень просты. Фактически каждый нейрон слоя Гроссберга лишь выдает величину веса, который связывает этот нейрон с единственным ненулевым нейроном Кохонена.\n",
    "\n",
    "**Обучение слоя Кохонена.** Слой Кохонена классифицирует входные векторы в группы схожих векторов.\n",
    "\n",
    "Это достигается с помощью такой подстройки весов слоя Кохонена, что близкие входные векторы активируют один и тот же нейрон данного слоя. Задачей слоя Гроссберга является получение требуемых выходов.\n",
    "\n",
    "Обучение Кохонена является самообучением, протекающим без учителя. Поэтому трудно (и не нужно) предсказывать, какой именно нейрон Кохонена будет активироваться для заданного входного вектора. Необходимо лишь гарантировать, чтобы в результате обучения разделялись несхожие входные векторы.\n",
    "\n",
    "### Предварительная обработка входных векторов\n",
    "\n",
    "В алгебраической форме записи имеем\n",
    "\n",
    "<img src=\"expression4.png\" width=\"150\"/>\n",
    "\n",
    "Это превращает входной вектор в единичный вектор с тем же самым направлением, т. е. в вектор единичной длины в n-мерном пространстве. Приведенное уравнение обобщает случай двух измерений, когда длина вектора равна гипотенузе прямоугольного треугольника, образованного его х и у компонентами, как это следует из теоремы Пифагора.\n",
    "\n",
    "При обучении слоя Кохонена на вход подается входной вектор и вычисляются его скалярные произведения с векторами весов, связанными со всеми нейронами Кохонена. Нейрон с максимальным значением скалярного произведения объявляется «победителем» и его веса подстраиваются.\n",
    "\n",
    "Уравнение, описывающее процесс обучения имеет следующий вид:\n",
    "\n",
    "<img src=\"expression5.png\" width=\"150\"/>\n",
    "\n",
    "где w<sub>н</sub> - новое значение веса, соединяющего входную компоненту х с выигравшим нейроном; w<sub>с</sub> - предыдущее значение этого веса; а - коэффициент скорости обучения, который может варьироваться в процессе обучения.\n",
    "\n",
    "Каждый вес, связанный с выигравшим нейроном Кохонена, изменяется пропорционально разности между его величиной и величиной входа, к которому он присоединен. Направление изменения минимизирует разность между весом и его входом.\n",
    "\n",
    "### Выбор начальных значений весовых векторов\n",
    "\n",
    "Всем весам сети перед началом обучения следует придать начальные значения. Общепринятой практикой при работе с нейронными сетями является присваивание весам небольших случайных значений. При обучении слоя Кохонена случайно выбранные весовые векторы следует нормализовать. Окончательные значения весовых векторов после обучения совпадают с нормализованными входными векторами. Поэтому нормализация перед началом обучения приближает весовые векторы к их окончательным значениям, сокращая, таким образом, обучающий процесс. Рандомизация весов слоя Кохонена может породить серьезные проблемы при обучении, так как в результате ее весовые векторы распределяются равномерно по поверхности гиперсферы. Из-за того, что входные векторы, как правило, распределены неравномерно и имеют тенденцию группироваться на относительно малой части поверхности гиперсферы, большинство весовых векторов будут так удалены от любого входного вектора, что они никогда не будут давать наилучшего соответствия. Эти нейроны Кохонена будут всегда иметь нулевой выход и окажутся бесполезными. Более того, оставшихся весов, дающих наилучшие соответствия, может оказаться слишком мало, чтобы разделить входные векторы на классы, которые расположены близко друг к другу на поверхности гиперсферы. Допустим, что имеется несколько множеств входных векторов, все множества сходные, но должны быть разделены на различные классы. Сеть должна быть обучена активировать отдельный нейрон Кохонена для каждого класса. Если начальная плотность весовых векторов в окрестности обучающих векторов слишком мала, то может оказаться невозможным разделить сходные классы из-за того, что не будет достаточного количества весовых векторов в интересующей нас окрестности, чтобы приписать по одному из них каждому классу входных векторов. Наоборот, если несколько входных векторов получены незначительными изменениями из одного и того же образца и должны быть объединены в один класс, то они должны включать один и тот же нейрон Кохонена. Если же плотность весовых векторов очень высока вблизи группы слегка различных входных векторов, то каждый входной вектор может активировать отдельный нейрон Кохонена. Это не является катастрофой, так как слой Гроссберга может отобразить различные нейроны Кохонена в один и тот же выход, но это расточительная трата нейронов Кохонена. Наиболее желательное решение состоит в том, чтобы распределять весовые векторы в соответствии с плотностью входных векторов, которые должны быть разделены, помещая тем самым больше весовых векторов в окрестности большого числа входных векторов. На практике это невыполнимо, однако существует несколько методов приближенного достижения тех же целей. Одно из решений, известное под названием метода выпуклой комбинации (convex combination method), состоит в том, что все веса приравниваются одной и той же величине\n",
    "\n",
    "<img src=\"expression6.png\" width=\"100\"/>\n",
    "\n",
    "где n - число входов и, следовательно, число компонент каждого весового вектора. Благодаря этому все весовые векторы совпадают и имеют единичную длину. Каждой же компоненте входа Х придается значение\n",
    "\n",
    "<img src=\"expression7.png\" width=\"120\"/>\n",
    "\n",
    "где n - число входов. В начале a очень мало, вследствие чего все входные векторы имеют длину, близкую к 1/n½, и почти совпадают с векторами весов. В процессе обучения сети a постепенно возрастает, приближаясь к единице. Это позволяет разделять входные векторы и окончательно приписывает им их истинные значения. Весовые векторы отслеживают один или небольшую группу входных векторов и в конце обучения дают требуемую картину выходов. Метод выпуклой комбинации хорошо работает, но замедляет процесс обучения, так как весовые векторы подстраиваются к изменяющейся цели. Другой подход состоит в добавлении шума к входным векторам. Тем самым они подвергаются случайным изменениям, схватывая в конце концов весовой вектор. Этот метод также работоспособен, но еще более медленен, чем метод выпуклой комбинации. Третий метод начинает со случайных весов, но на начальной стадии обучающего процесса подстраивает все веса, а не только связанные с выигравшим нейроном Кохонена. Тем самым весовые векторы перемещаются ближе к области входных векторов. В процессе обучения коррекция весов начинает производиться лишь для ближайших к победителю нейронов Кохонена. Этот радиус коррекции постепенно уменьшается так, что в конце концов корректируются только веса, связанные с выигравшим нейроном Кохонена. Еще один метод наделяет каждый нейрон Кохонена \"Чувством справедливости\". Если он становится победителем чаще своей законной доли времени (примерно 1/k, где k - число нейронов Кохонена), он временно увеличивает свой порог, что уменьшает его шансы на выигрыш, давая тем самым возможность обучаться и другим нейронам. Во многих приложениях точность результата существенно зависит от распределения весов. К сожалению, эффективность различных решений исчерпывающим образом не оценена и остается проблемой. \n",
    "\n",
    "### Режим интерполяции\n",
    "\n",
    "До сих пор мы обсуждали алгоритм обучения, в котором для каждого входного вектора активировался лишь один нейрон Кохонена. Это называется методом аккредитации. Его точность ограничена, так как выход полностью является функцией лишь одного нейрона Кохонена. В методе интерполяции целая группа нейронов Кохонена, имеющих наибольшие выходы, может передавать свои выходные сигналы в слой Гроссберга. Число нейронов в такой группе должно выбираться в зависимости от задачи, и убедительных данных относительно оптимального размера группы не имеется. Как только группа определена, ее множество выходов NET рассматривается как вектор, длина которого нормализуется на единицу делением каждого значения NET на корень квадратный из суммы квадратов значений NET в группе. Все нейроны вне группы имеют нулевые выходы. Метод интерполяции способен устанавливать более сложные соответствия и может давать более точные результаты. По-прежнему, однако, нет убедительных данных, позволяющих сравнить режимы интерполяции и аккредитации. \n",
    "\n",
    "### Обучение слоя Гроссберга\n",
    "\n",
    "Слой Гроссберга обучается относительно просто. Входной вектор, являющийся выходом слоя Кохонена, подается на слой нейронов Гроссберга, и выходы слоя Гроссберга вычисляются, как при нормальном функционировании. Далее, каждый вес корректируется лишь в том случае, если он соединен с нейроном Кохонена, имеющим ненулевой выход. Величина коррекции веса пропорциональна разности между весом и требуемым выходом нейрона Гроссберга, с которым он соединен. В символьной записи\n",
    "\n",
    "<img src=\"expression8.png\" width=\"120\"/>\n",
    "\n",
    "где k<sub>i</sub> - выход i-го нейрона Кохонена (только для одного нейрона Кохонена он отличен от нуля); у<sub>j</sub> - j-ая компонента вектора желаемых выходов. Первоначально ß берется равным ~0,1 и затем постепенно уменьшается в процессе обучения. Отсюда видно, что веса слоя Гроссберга будут сходиться к средним величинам от желаемых выходов, тогда как веса слоя Кохонена обучаются на средних значениях входов. Обучение слоя Гроссберга - это обучение с учителем, алгоритм располагает желаемым выходом, по которому он обучается. Обучающийся без учителя, самоорганизующийся слой Кохонена дает выходы в недетерминированных позициях. Они отображаются в желаемые выходы слоем Гроссберга. \n",
    "\n",
    "## Сеть встречного распространения полностью\n",
    "\n",
    "Сеть встречного распространения целиком. В режиме нормального функционирования предъявляются входные векторы Х и Y, и обученная сеть дает на выходе векторы X' и Y', являющиеся аппроксимациями соответственно для Х и Y. Векторы Х и Y предполагаются здесь нормализованными единичными векторами, следовательно, порождаемые на выходе векторы также будут иметь тенденцию быть нормализованными. В процессе обучения векторы Х и Y подаются одновременно и как входные векторы сети, и как желаемые выходные сигналы. Вектор Х используется для обучения выходов X', а вектор Y - для обучения выходов Y' слоя Гроссберга. Сеть встречного распространения целиком обучается с использованием того же самого метода, который описывался для сети прямого действия. Нейроны Кохонена принимают входные сигналы как от векторов X, так и от векторов Y. Но это неотличимо от ситуации, когда имеется один большой вектор, составленный из векторов Х и Y, и не влияет на алгоритм обучения.\n",
    "\n",
    "В качестве результирующего получается единичное отображение, при котором предъявление пары входных векторов порождает их копии на выходе. Это не представляется особенно интересным, если не заметить, что предъявление только вектора Х (с вектором Y, равным нулю) порождает как выходы X', так и выходы Y'. Если F - функция, отображающая Х в Y', то сеть аппроксимирует ее. Также, если F обратима, то предъявление только вектора Y (приравнивая Х нулю) порождает X'. Уникальная способность порождать функцию и обратную к ней делает сеть встречного распространения полезной в ряде приложений. Рис. 4.2 в отличие от первоначальной конфигурации не демонстрирует противоток в сети, по которому она получила свое название.\n",
    "\n",
    "<img src=\"forward_propagation_compl.gif\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd0bc4-764a-43fa-902a-9dfbb43bd29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d3467-85c3-4db9-80db-7e92c06523c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6690249-8718-4a24-9b7d-b95eb9270f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
