{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сеть встречного распространения\n",
    "\n",
    "## Введение\n",
    "\n",
    "Сеть встречного распространения (Contrastive Divergence, CD) - это метод, используемый в обучении ограниченных машин Больцмана и глубоких нейронных сетей. Этот метод предназначен для ускорения процесса обучения, позволяя модели быстро сходиться к локальному оптимуму.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные особенности\n",
    "\n",
    "1. Обучаются без учителя, т.к. нейроны сети не требуют маркированных входных данных для обучения, а вместо этого учатся распознавать и воспроизводить паттерны в данных.\n",
    "\n",
    "2. Сеть состоит из входного слоя, выходного слоя и одного или нескольких слоев ассоциативных нейронов между ними, то есть сеть обладает двухслойной архитектурой.\n",
    "\n",
    "3. Веса нейронов корректируются для минимизации ошибки на выходе, при этом корректировка происходит одновременно для всех нейронов.\n",
    "\n",
    "4. Могут быть использованы в решении различных задач, связанных с распознаванием образов, сжатием данных и ассоциативной памятью.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Структура сети\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Структура сети\n",
    "\n",
    "* Нейроны слоя 0 (показанные кружками) служат лишь точками разветвления и не выполняют вычислений\n",
    "* Каждый нейрон слоя 0 соединен с каждым нейроном слоя 1 (называемого слоем Кохонена) отдельным весом w<sub>mn</sub> (веса в целом рассматриваются как матрица весов W)\n",
    "* Аналогично, каждый нейрон в слое Кохонена (слое 1) соединен с каждым нейроном в слое Гроссберга (слое 2) весом v<sub>np</sub> (веса образуют матрицу весов V)\n",
    "\n",
    "<img src=\"counterpropagation_network.png\" width=\"500\"/>\n",
    "\n",
    "Особенность состоит в операциях, выполняемых нейронами Кохонена и Гроссберга. Как и многие другие сети, встречное распространение функционирует в двух режимах: в нормальном режиме, при котором принимается входной вектор X и выдается выходной вектор Y, и в режиме обучения, при котором подается входной вектор и веса корректируются, чтобы дать требуемый выходной вектор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормальное функционирование\n",
    "\n",
    "### Слои Кохонена\n",
    "\n",
    "В своей простейшей форме слой Кохонена функционирует так, что для данного входного вектора один и только один нейрон Кохонена выдает на выходе логическую единицу, все остальные выдают ноль. \n",
    "\n",
    "Нейроны Кохонена можно воспринимать как набор электрических лампочек, так что для любого входного вектора загорается одна из них. Ассоциированное с каждым нейроном Кохонена множество весов соединяет его с каждым входом.\n",
    "\n",
    "Выход каждого нейрона Коханена высчитывается как:\n",
    "\n",
    "$\n",
    "NET_i = w_{1j}x_1 + w_{2j}x_2 + ... + w_{mj}x_m\n",
    "$ \n",
    "\n",
    "где NET<sub>j</sub> - это выход NET нейрона Коханена \n",
    "\n",
    "$\n",
    "N = XW\n",
    "$\n",
    "\n",
    "где **N** - вектор выходов NET слоя Кохонена\n",
    "\n",
    "Нейрон Кохонена с максимальным значением NET является \"победителем\". Его выход равен единице, у остальных он равен нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормальное функционирование\n",
    "\n",
    "### Слой  Гроссберга\n",
    "\n",
    "Слой Гроссберга функционирует в сходной манере. Его выход NET является взвешенной суммой выходов k<sub>1</sub>,k<sub>2</sub>, ..., k<sub>n</sub> слоя Кохонена, образующих вектор К. Вектор соединяющих весов, обозначенный через V, состоит из весов v<sub>11</sub>, v<sub>21</sub>, ..., v<sub>np</sub>. Тогда выход NET каждого нейрона Гроссберга есть:\n",
    "\n",
    "$\n",
    "NET_j = \\sum^n_ik_iw_{ij}\n",
    "$\n",
    "\n",
    "где NET<sub>j</sub> - выход j-го нейрона Гроссберга, или в векторной форме\n",
    "\n",
    "$\n",
    "Y = KV\n",
    "$\n",
    "\n",
    "где Y - выходной вектор слоя Гроссберга, K - выходной вектор слоя Кохонена, V - матрица весов слоя Гроссберга. Если слой Кохонена функционирует таким образом, что лишь у одного нейрона величина NET равна единице, а у остальных равна нулю, то лишь один элемент вектора K отличен от нуля, и вычисления очень просты. Фактически каждый нейрон слоя Гроссберга лишь выдает величину веса, который связывает этот нейрон с единственным ненулевым нейроном Кохонена. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение слоя Кохонена\n",
    "\n",
    "1. Входной сигнал подается на входной слой нейронов, которые передают сигнал на слой Кохонена\n",
    "2. На слое Кохонена каждый нейрон имеет свой вес (координату в пространстве признаков), который определяет его положение на карте Кохонена. Эти веса представляют собой матрицу, где каждая строка соответствует одному нейрону, а каждый столбец соответствует одной координате в пространстве признаков.\n",
    "3. Каждый нейрон на слое Кохонена вычисляет свое расстояние до входного сигнала, используя функцию расстояния, такую как евклидово расстояние. Нейрон с наименьшим расстоянием называется победителем или наиболее активным нейроном\n",
    "4. Веса нейронов на слое Кохонена обновляются в направлении к входному сигналу, используя алгоритм обучения Кохонена, такой как обучение с конкуренцией. В результате карта Кохонена самоорганизуется и разбивает пространство признаков на кластеры\n",
    "5. Выходной слой нейронов получает информацию о наиболее активном нейроне на слое Кохонена и выводит соответствующий класс или метку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение слоя Гроссберга\n",
    "\n",
    "1. Входной сигнал распространяется по слою Гроссберга, и каждый нейрон вычисляет свой выходной сигнал, используя свою функцию активации\n",
    "2. Ошибка вычисляется на выходном слое, и она распространяется обратно через сеть в направлении входного слоя\n",
    "3. Веса связей между нейронами корректируются таким образом, чтобы уменьшить ошибку на выходном слое. Это достигается путем изменения весов в соответствии с градиентом ошибки, который вычисляется с использованием алгоритма обратного распространения ошибки\n",
    "4. Этот процесс повторяется для каждого входного сигнала и соответствующей целевой величины, пока не будет достигнута требуемая точность или количество эпох обучения не будет выполнено"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
