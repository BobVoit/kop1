{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сеть встречного распространения\n",
    "\n",
    "## Введение\n",
    "\n",
    "Сеть встречного распространения (Contrastive Divergence, CD) - это метод, используемый в обучении ограниченных машин Больцмана и глубоких нейронных сетей. Этот метод предназначен для ускорения процесса обучения, позволяя модели быстро сходиться к локальному оптимуму.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные особенности\n",
    "\n",
    "1. Во встречном распространении объединены два алгоритма: самоорганизующаяся карта Кохонена и звезда Гроссберга. Их объединение ведет к свойствам, которых нет ни у одного из них в отдельности\n",
    "\n",
    "2. Возможности сети встречного распространения, превосходят возможности однослойных сетей. Время же обучения по сравнению с обратным распространением может уменьшаться в сто раз\n",
    "\n",
    "3. Веса нейронов корректируются для минимизации ошибки на выходе, при этом корректировка происходит одновременно для всех нейронов.\n",
    "\n",
    "4. Сеть позволяет получать правильный выход даже при приложении входного вектора, который является неполным или слегка неверным. Это позволяет использовать данную сеть для распознавания образов, восстановления образов и усиления сигналов.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Структура сети\n",
    "\n",
    "* Нейроны слоя 0 (показанные кружками) служат лишь точками разветвления и не выполняют вычислений\n",
    "* Каждый нейрон слоя 0 соединен с каждым нейроном слоя 1 (называемого слоем Кохонена) отдельным весом w<sub>mn</sub> (веса в целом рассматриваются как матрица весов W)\n",
    "* Аналогично, каждый нейрон в слое Кохонена (слое 1) соединен с каждым нейроном в слое Гроссберга (слое 2) весом v<sub>np</sub> (веса образуют матрицу весов V)\n",
    "\n",
    "<img src=\"counterpropagation_network.png\" width=\"500\"/>\n",
    "\n",
    "Особенность состоит в операциях, выполняемых нейронами Кохонена и Гроссберга. Как и многие другие сети, встречное распространение функционирует в двух режимах: в нормальном режиме, при котором принимается входной вектор X и выдается выходной вектор Y, и в режиме обучения, при котором подается входной вектор и веса корректируются, чтобы дать требуемый выходной вектор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормальное функционирование\n",
    "\n",
    "### Слои Кохонена\n",
    "\n",
    "В своей простейшей форме слой Кохонена функционирует так, что для данного входного вектора один и только один нейрон Кохонена выдает на выходе логическую единицу, все остальные выдают ноль. \n",
    "\n",
    "Нейроны Кохонена можно воспринимать как набор электрических лампочек, так что для любого входного вектора загорается одна из них. Ассоциированное с каждым нейроном Кохонена множество весов соединяет его с каждым входом.\n",
    "\n",
    "Выход каждого нейрона Коханена высчитывается как:\n",
    "\n",
    "$\n",
    "NET_i = w_{1j}x_1 + w_{2j}x_2 + ... + w_{mj}x_m\n",
    "$ \n",
    "\n",
    "где NET<sub>j</sub> - это выход NET нейрона Коханена \n",
    "\n",
    "$\n",
    "N = XW\n",
    "$\n",
    "\n",
    "где **N** - вектор выходов NET слоя Кохонена\n",
    "\n",
    "Нейрон Кохонена с максимальным значением NET является \"победителем\". Его выход равен единице, у остальных он равен нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормальное функционирование\n",
    "\n",
    "### Слой  Гроссберга\n",
    "\n",
    "Слой Гроссберга функционирует в сходной манере. Его выход NET является взвешенной суммой выходов k<sub>1</sub>,k<sub>2</sub>, ..., k<sub>n</sub> слоя Кохонена, образующих вектор К. Вектор соединяющих весов, обозначенный через V, состоит из весов v<sub>11</sub>, v<sub>21</sub>, ..., v<sub>np</sub>. Тогда выход NET каждого нейрона Гроссберга есть:\n",
    "\n",
    "$\n",
    "NET_j = \\sum^n_ik_iw_{ij}\n",
    "$\n",
    "\n",
    "где NET<sub>j</sub> - выход j-го нейрона Гроссберга, или в векторной форме\n",
    "\n",
    "$\n",
    "Y = KV\n",
    "$\n",
    "\n",
    "где Y - выходной вектор слоя Гроссберга, K - выходной вектор слоя Кохонена, V - матрица весов слоя Гроссберга. Если слой Кохонена функционирует таким образом, что лишь у одного нейрона величина NET равна единице, а у остальных равна нулю, то лишь один элемент вектора K отличен от нуля, и вычисления очень просты. Фактически каждый нейрон слоя Гроссберга лишь выдает величину веса, который связывает этот нейрон с единственным ненулевым нейроном Кохонена. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение слоя Кохонена\n",
    "\n",
    "<!-- 1. Входной сигнал подается на входной слой нейронов, которые передают сигнал на слой Кохонена\n",
    "2. На слое Кохонена каждый нейрон имеет свой вес (координату в пространстве признаков), который определяет его положение на карте Кохонена. Эти веса представляют собой матрицу, где каждая строка соответствует одному нейрону, а каждый столбец соответствует одной координате в пространстве признаков.\n",
    "3. Каждый нейрон на слое Кохонена вычисляет свое расстояние до входного сигнала, используя функцию расстояния, такую как евклидово расстояние. Нейрон с наименьшим расстоянием называется победителем или наиболее активным нейроном\n",
    "4. Веса нейронов на слое Кохонена обновляются в направлении к входному сигналу, используя алгоритм обучения Кохонена, такой как обучение с конкуренцией. В результате карта Кохонена самоорганизуется и разбивает пространство признаков на кластеры\n",
    "5. Выходной слой нейронов получает информацию о наиболее активном нейроне на слое Кохонена и выводит соответствующий класс или метку -->\n",
    "\n",
    "Обучение Кохонена является самообучением, протекающим без учителя. Поэтому трудно (и не нужно) предсказывать, какой именно нейрон Кохонена будет активироваться для заданного входного вектора. Необходимо лишь гарантировать, чтобы в результате обучения разделялись несхожие входные векторы.\n",
    "\n",
    "**Как проходит обучение слоя?**\n",
    "\n",
    "1. Требуется нормализовать входные векторы перед тем, как предъявлять их сети (хотя не обязательно). Это выполняется с помощью деления каждой компоненты входного вектора на длину вектора. Эта длина находится извлечением квадратного корня из суммы квадратов компонент вектора\n",
    "\n",
    "$$\n",
    "x'_i = \\frac{x_i}{\\sqrt{x^2_1 + x^2_2 + ... + x^2_n}}\n",
    "$$\n",
    "\n",
    "На выходе получается единичный вектор с тем же направлением\n",
    "\n",
    "2. На вход слоя Коханена подается входной вектор и вычисляются его скалярные произведения с векторами весов, связанными со всеми нейронами Кохонена. Нейрон с максимальным значением скалярного произведения объявляется \"победителем\" и его веса подстраиваются. Скалярное произведение, используемое для вычисления величин NET, является мерой сходства между входным вектором и вектором весов, то процесс обучения состоит в выборе нейрона Кохонена с весовым вектором, наиболее близким к входному вектору, и дальнейшем приближении весового вектора к входному.\n",
    "\n",
    "Уравнение, описывающее процесс обучения имеет следующий вид:\n",
    "\n",
    "$$\n",
    "w_н = w_c + a(x - w_c)\n",
    "$$\n",
    "\n",
    "w_н - новое значение веса, соединяющего входную компоненту х с выигравшим нейроном;\n",
    "\n",
    "w_c - предыдущее значение этого веса;\n",
    "\n",
    "a - коэффициент скорости обучения (может варьироваться в процессе обучения); \n",
    "\n",
    "Каждый вес, связанный с выигравшим нейроном Кохонена, изменяется пропорционально разности между его величиной и величиной входа, к которому он присоединен. Направление изменения минимизирует разность между весом и его входом.\n",
    "\n",
    "Переменная к является коэффициентом скорости обучения, который вначале обычно равен ~ 0,7 и может постепенно уменьшаться в процессе обучения. Это позволяет делать большие начальные шаги для быстрого грубого обучения и меньшие шаги при подходе к окончательной величине. Если бы с каждым нейроном Кохонена ассоциировался один входной вектор, то слой Кохонена мог бы быть обучен с помощью одного вычисления на вес. Веса нейрона-победителя приравнивались бы к компонентам обучающего вектора (a = 1). Как правило, обучающее множество включает много сходных между собой входных векторов, и сеть должна быть обучена активировать один и тот же нейрон Кохонена для каждого из них. В этом случае веса, этого нейрона должны получаться усреднением входных векторов, которые должны его активировать. Постепенное уменьшение величины a уменьшает воздействие каждого обучающего шага, так что окончательное значение будет средней величиной от входных векторов, на которых происходит обучение. Таким образом, веса, ассоциированные с нейроном, примут значение вблизи \"центра\" входных векторов, для которых данный нейрон является \"победителем\".\n",
    "\n",
    "**Выбор начальных значений весовых векторов**\n",
    "\n",
    "В качестве начальных значений для весов берутся небольшие случайные числа, которые сперва нормализуются. Окончательные значения весовых векторов после обучения совпадают с нормализованными входными векторами. Рандомизация весов слоя Кохонена может породить серьезные проблемы при обучении, так как в результате ее весовые векторы распределяются равномерно по поверхности гиперсферы. Из-за того, что входные векторы, как правило, распределены неравномерно и имеют тенденцию группироваться на относительно малой части поверхности гиперсферы, большинство весовых векторов будут так удалены от любого входного вектора, что они никогда не будут давать наилучшего соответствия. Эти нейроны Кохонена будут всегда иметь нулевой выход и окажутся бесполезными. Более того, оставшихся весов, дающих наилучшие соответствия, может оказаться слишком мало, чтобы разделить входные векторы на классы, которые расположены близко друг к другу на поверхности гиперсферы. Допустим, что имеется несколько множеств входных векторов, все множества сходные, но должны быть разделены на различные классы. Сеть должна быть обучена активировать отдельный нейрон Кохонена для каждого класса. Если начальная плотность весовых векторов в окрестности обучающих векторов слишком мала, то может оказаться невозможным разделить сходные классы из-за того, что не будет достаточного количества весовых векторов в интересующей нас окрестности, чтобы приписать по одному из них каждому классу входных векторов. Наоборот, если несколько входных векторов получены незначительными изменениями из одного и того же образца и должны быть объединены в один класс, то они должны включать один и тот же нейрон Кохонена. Если же плотность весовых векторов очень высока вблизи группы слегка различных входных векторов, то каждый входной вектор может активировать отдельный нейрон Кохонена. Это не является катастрофой, так как слой Гроссберга может отобразить различные нейроны Кохонена в один и тот же выход, но это расточительная трата нейронов Кохонена. Наиболее желательное решение состоит в том, чтобы распределять весовые векторы в соответствии с плотностью входных векторов, которые должны быть разделены, помещая тем самым больше весовых векторов в окрестности большого числа входных векторов. На практике это невыполнимо, однако существует несколько методов приближенного достижения тех же целей. Одно из решений, известное под названием метода выпуклой комбинации, состоит в том, что все веса приравниваются одной и той же величине\n",
    "\n",
    "$$\n",
    "w_i = \\frac{1}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "\n",
    "n - число входов и, следовательно, число компонент каждого весового вектора.\n",
    "\n",
    "Каждой компоненте X придается значение:\n",
    "\n",
    "$$\n",
    "x_i = ax_i + \\frac{(1 - a)}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "n - число входов;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение слоя Гроссберга\n",
    "\n",
    "<!-- 1. Входной сигнал распространяется по слою Гроссберга, и каждый нейрон вычисляет свой выходной сигнал, используя свою функцию активации\n",
    "2. Ошибка вычисляется на выходном слое, и она распространяется обратно через сеть в направлении входного слоя\n",
    "3. Веса связей между нейронами корректируются таким образом, чтобы уменьшить ошибку на выходном слое. Это достигается путем изменения весов в соответствии с градиентом ошибки, который вычисляется с использованием алгоритма обратного распространения ошибки\n",
    "4. Этот процесс повторяется для каждого входного сигнала и соответствующей целевой величины, пока не будет достигнута требуемая точность или количество эпох обучения не будет выполнено -->\n",
    "\n",
    "1. Выходной вектор слоя Кохонена подается на слой нейронов Гроссберга, и выходы слоя Гроссберга вычисляются.\n",
    "2. Далее, каждый вес корректируется лишь в том случае, если он соединен с нейроном Кохонена, имеющим ненулевой выход. Величина коррекции веса:\n",
    "\n",
    "$$\n",
    "v_{ijн} = v_{ijc} + \\beta(y_j - v_ijc)k_i\n",
    "$$\n",
    "k_i - выход i-го нейрона Кохонена (только для одного нейрона Кохонена он отличен от нуля); \n",
    "\n",
    "у_j - j-ая компонента вектора желаемых выходов\n",
    "\n",
    "Первоначально beta ~ 0.1 и затем постепенно уменьшается в процессе обучения. Веса слоя Гроссберга будут сходиться к средним величинам от желаемых выходов, тогда как веса слоя Кохонена обучаются на средних значениях входов. Обучение слоя Гроссберга - это обучение с учителем, алгоритм располагает желаемым выходом, по которому он обучается. Обучающийся без учителя, самоорганизующийся слой Кохонена дает выходы в недетерминированных позициях. Они отображаются в желаемые выходы слоем Гроссберга. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
